{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZbEOXBUsiNg"
   },
   "source": [
    "# An introduction to PyTorch\n",
    "\n",
    "PyTorch is an optimized tensor library for deep learning using GPUs and CPUs, often used in state-of-the-art research. (Do not fear if you do not yet know what a tensor is, it will become clear in this small tutorial.)\n",
    "During this lab, we will first explain and demonstrate how PyTorch works, after which you will build both a linear regression model and a classifier on MNIST digits!\n",
    "\n",
    "For more information on PyTorch, you can visit the [official documentation](https://pytorch.org/docs/stable/index.html). Do not hesitate to contact us during the lab, or via mail when something is not clear:\n",
    "*   benoit.devrieze@uantwerpen.be\n",
    "*   stijn.vanraemdonck@uantwerpen.be\n",
    "\n",
    "The content of this notebook is inspired by the [Learn the Basics](https://pytorch.org/tutorials/beginner/basics/intro.html) series. Other tutorials can be found there as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFk7t7HxyALQ"
   },
   "source": [
    "## Tensors\n",
    "As promised, we will take a look at tensors. Tensors are a specialized data structure that is very similar to arrays and matrices. More specific, a Torch tensor is a multi-dimensional matrix containing elements of a single data type. In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.\n",
    "\n",
    "Before we can start using these tensors, we need to import the correct libraries. To run the code below, click on the light-grey box and press \"ctrl + enter\". You can also run text fields after accidentally going to edit mode, such that it will be parsed (shift + enter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T06:43:54.974219400Z",
     "start_time": "2024-10-25T06:43:54.964220500Z"
    },
    "id": "1kZbq07Msh36"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PzXQNPcxHOf"
   },
   "source": [
    "### Initializing a Tensor\n",
    "Tensors can be initialized in various ways and with various data types. Take a look at the following examples:\n",
    "\n",
    "**Creating a Tensor directly from data**\n",
    "\n",
    "Tensors can be created directly from data. The data type is automatically inferred (in this example Torch infers the data type as integer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T06:44:04.220384900Z",
     "start_time": "2024-10-25T06:44:04.180235300Z"
    },
    "id": "gEGxUEb5yhZP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "torch.int64\n",
      "[[1, 2], [3, 4]]\n"
     ]
    }
   ],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(x_data)\n",
    "print(x_data.dtype)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJgVD0K6ysBG"
   },
   "source": [
    "**Creating a Tensor from a NumPy array**\n",
    "\n",
    "Tensors can be created from NumPy arrays (and vice versa).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T06:44:09.534280200Z",
     "start_time": "2024-10-25T06:44:09.524279100Z"
    },
    "id": "DCamz_scyuQ4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n",
      "torch.int32\n"
     ]
    }
   ],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(x_np)\n",
    "print(x_np.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bung9CoR0zFv"
   },
   "source": [
    "**Creating a Tensor from another tensor:**\n",
    "\n",
    "The new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T06:44:15.232867400Z",
     "start_time": "2024-10-25T06:44:15.172578500Z"
    },
    "id": "ajaXjQDe03IS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our base tensor: \n",
      " tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int16) \n",
      " with shape: \n",
      " torch.Size([2, 2]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]], dtype=torch.int16) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.8373, 0.0394],\n",
      "        [0.2131, 0.4713]]) \n",
      " with data type: \n",
      " torch.float32\n"
     ]
    }
   ],
   "source": [
    "x_temp = torch.ShortTensor([[1, 2], [3, 4], ])  # as an example, we will create a 2D-tensor, with data type equaling a signed 16-bit Integer\n",
    "print(f\"Our base tensor: \\n {x_temp} \\n with shape: \\n {x_temp.shape} \\n\")\n",
    "\n",
    "x_ones = torch.ones_like(x_temp) # retains the properties of x_temp\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_temp, dtype=torch.float) # overrides the datatype of x_temp\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n with data type: \\n {x_rand.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRzgh49x2hx4"
   },
   "source": [
    "**Creating a Tensor with random or constant values:**\n",
    "\n",
    "``shape`` is a tuple of tensor dimensions. In the functions below, it determines the dimensionality of the output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T06:44:27.387546Z",
     "start_time": "2024-10-25T06:44:27.357546200Z"
    },
    "id": "EtbBj1bJ2jDc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.1345, 0.7871, 0.3400],\n",
      "        [0.0941, 0.2881, 0.3943]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5W-PJzScyrx1"
   },
   "source": [
    "**Alternative ways to create a tensor**\n",
    "\n",
    "You've now seen that there are many ways to create a tensor. There are however many alternatives to these methods.\n",
    "For example: if you want a tensor with a 32-bit floating point data type (often abbreviated as dtype), you can create it as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T06:44:31.084076400Z",
     "start_time": "2024-10-25T06:44:31.074076900Z"
    },
    "id": "O4ND-0kKsWUL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.4366e+22, 5.3540e+22],\n",
      "        [1.0525e+21, 1.0741e-05],\n",
      "        [5.4174e-05, 4.2726e-05]])\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.FloatTensor(3, 2)\n",
    "print(tensor)\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_Wnz_Z13sC1"
   },
   "source": [
    "If you're interrested in viewing other initialisation options, visit [this link](https://pytorch.org/docs/stable/tensors.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ajDGiaE4eVj"
   },
   "source": [
    "### Short visual explanation of a tensor\n",
    "*This part can be skipped if you have a thorough understanding of tensors.*\n",
    "\n",
    "#### One Dimension\n",
    "\n",
    "Lets take a look at two use cases of a 1D-tensor with 3 values, as we can still very easily visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T06:44:42.445817500Z",
     "start_time": "2024-10-25T06:44:42.425795600Z"
    },
    "id": "_OvjP-1b45Cm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0800, 0.9000, 0.0200])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "position_tensor = torch.FloatTensor([0.08, 0.9, 0.02])\n",
    "print(position_tensor)\n",
    "print(position_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kM5Emt4l6MMZ"
   },
   "source": [
    "Depending on the use case, this one dimensional tensor could either represent an (x, y, z) location or act as the final layer of a classification network. In the first case this can represent a point or a vector in space as follows:\n",
    "\n",
    "**Point**\n",
    "\n",
    "![point_image](images/point.png)\n",
    "\n",
    "**Vector**\n",
    "\n",
    "![vector_image](images/vector.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTvUe6YIEWWV"
   },
   "source": [
    "Imagine the latter use case, such that it classifies images in one of the following classes: [dog, cat, bird]. In that case, it shows how convinced the network is that a given input is a dog, cat or bird. E.g. if the output is [0.08, 0.9, 0.02], the network believes that the image contains a cat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjKNA0anExNB"
   },
   "source": [
    "#### Two and more dimensions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f02XZJuuAJdt"
   },
   "source": [
    "Once we go to higher dimensions, it might become difficult to understand what the tensor represents. Thats why we will create a quick and intuitive example.\n",
    "\n",
    "Lets create a method to visualize a tensor. To do this, we need to understand the shapes of PyTorch and matplotlib. PyTorch mostly work with shape (Batch size, Channels, Height, Width), whereas PyPlot works with shape (Width, Height, Channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T06:46:47.456836200Z",
     "start_time": "2024-10-25T06:46:47.446833800Z"
    },
    "id": "VZsE20IZEwZ2"
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_tensor(to_plot: torch.Tensor):\n",
    "  plt.imshow(transforms.ToPILImage()(to_plot), interpolation=\"None\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khDGh5lqCIGM"
   },
   "source": [
    "Now we will create a very small RGB image of shape (3, 2, 4), representing (RGB, Height, Width)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T06:46:53.404776600Z",
     "start_time": "2024-10-25T06:46:53.220456500Z"
    },
    "id": "eszgysx_BWwn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB image tensor: \n",
      " tensor([[[0., 0., 0., 0.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 1., 1.],\n",
      "         [0., 0., 1., 1.]],\n",
      "\n",
      "        [[0., 1., 0., 1.],\n",
      "         [0., 1., 0., 1.]]]), with shape:torch.Size([3, 2, 4]). \n",
      "\n",
      "Plotting this tensor: \n",
      " \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "blue_channel = [[0, 1, 0, 1],\n",
    "                [0, 1, 0, 1]]\n",
    "green_channel = [[0, 0, 1, 1],\n",
    "                 [0, 0, 1, 1]]\n",
    "red_channel = [[0, 0, 0, 0],\n",
    "               [1, 1, 1, 1]]\n",
    "rgb = [red_channel, green_channel, blue_channel]\n",
    "\n",
    "image = torch.FloatTensor(rgb)\n",
    "print(f\"RGB image tensor: \\n {image}, with shape:{image.shape}. \\n\")\n",
    "\n",
    "print(f\"Plotting this tensor: \\n \")\n",
    "plot_tensor(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlDUlUZzG6RB"
   },
   "source": [
    "As can be seen now, this high dimensional tensor is actually not that difficult to understand. It can be seen as three 2D matrixes of a certain size, where each matrix represents a certain color value. Those three matrices are then concatenated on the first dimension to represent an RGB image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rRTaBXEYHj9q"
   },
   "source": [
    "### Tensor operations\n",
    "Now that we understand what tensors are, we want to perform some operations with them. We have all the default operations (sum, difference, quotient, element-wise multiplication), as well as more advanced operations (sin, cos, tan, dot product, ...).\n",
    "\n",
    "see https://pytorch.org/docs/stable/tensors.html# for a list of all operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6zbtjPhqI3Fp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor 1:\n",
      "\ttensor([[1, 2],\n",
      "        [3, 4]])\n",
      "Tensor 2:\n",
      "\ttensor([[4, 3],\n",
      "        [2, 1]])\n",
      "The sum is:\n",
      "\ttensor([[5, 5],\n",
      "        [5, 5]])\n",
      "The difference is:\n",
      "\ttensor([[-3, -1],\n",
      "        [ 1,  3]])\n",
      "The element-wise product is:\n",
      "\ttensor([[4, 6],\n",
      "        [6, 4]])\n",
      "The division is:\n",
      "\ttensor([[0.2500, 0.6667],\n",
      "        [1.5000, 4.0000]])\n",
      "The product with a constant '5' is:\n",
      "\ttensor([[ 5, 10],\n",
      "        [15, 20]])\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.tensor([[1, 2],[3, 4]])\n",
    "tensor2 = torch.tensor([[4, 3],[2, 1]])\n",
    "print(f\"Tensor 1:\\n\\t{tensor1}\")\n",
    "print(f\"Tensor 2:\\n\\t{tensor2}\")\n",
    "\n",
    "sum = tensor1 + tensor2\n",
    "print(f\"The sum is:\\n\\t{sum}\")\n",
    "difference = tensor1 - tensor2\n",
    "print(f\"The difference is:\\n\\t{difference}\")\n",
    "product = tensor1 * tensor2\n",
    "print(f\"The element-wise product is:\\n\\t{product}\")\n",
    "quotient = tensor1 / tensor2\n",
    "print(f\"The division is:\\n\\t{quotient}\")\n",
    "\n",
    "product_w_constant = tensor1 * 5\n",
    "print(f\"The product with a constant '5' is:\\n\\t{product_w_constant}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCGNOSQPrMpL"
   },
   "source": [
    "### Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcoAQ9eVKdyB"
   },
   "source": [
    "Something very usefull and important to keep in mind is that PyTorch can **broadcast** tensors. In short, if a PyTorch operation supports broadcast, then its Tensor arguments can be automatically expanded to be of equal sizes (without making copies of the data).\n",
    "\n",
    "Two tensors are “broadcastable” if the following rules hold:\n",
    "- Each tensor has at least one dimension.\n",
    "- When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.\n",
    "\n",
    "*Source https://pytorch.org/docs/stable/notes/broadcasting.html*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "O8FDClADLAzd"
   },
   "outputs": [],
   "source": [
    "x=torch.empty(5,7,3)\n",
    "y=torch.empty(5,7,3)\n",
    "# same shapes are always broadcastable (i.e. the above rules always hold)\n",
    "\n",
    "x=torch.empty((0,))\n",
    "y=torch.empty(2,2)\n",
    "# x and y are not broadcastable, because x does not have at least 1 dimension\n",
    "\n",
    "# can line up trailing dimensions\n",
    "x=torch.empty(5,3,4,1)\n",
    "y=torch.empty(  3,1,1)\n",
    "# x and y are broadcastable.\n",
    "# 1st trailing dimension: both have size 1\n",
    "# 2nd trailing dimension: y has size 1\n",
    "# 3rd trailing dimension: x size == y size\n",
    "# 4th trailing dimension: y dimension doesn't exist\n",
    "\n",
    "# but:\n",
    "x=torch.empty(5,2,4,1)\n",
    "y=torch.empty(  3,1,1)\n",
    "# x and y are not broadcastable, because in the 3rd trailing dimension 2 != 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2wL0I6BLhEr"
   },
   "source": [
    "If two tensors x, y are “broadcastable”, the resulting tensor size is calculated as follows:\n",
    "- If the number of dimensions of x and y are not equal, prepend 1 to the dimensions of the tensor with fewer dimensions to make them equal length.\n",
    "- Then, for each dimension size, the resulting dimension size is the max of the sizes of x and y along that dimension.\n",
    "\n",
    "For Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "z2SFnovqLSPu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These two tensors are broadcastable \n",
      " The resulting size is: torch.Size([5, 3, 4, 1]) \n",
      "\n",
      "These two tensors are broadcastable \n",
      " The resulting size is: torch.Size([3, 1, 7]) \n",
      "\n",
      "These two tensors are not broadcastable \n",
      " Adding them together gives us an error:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m y\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThese two tensors are not broadcastable \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Adding them together gives us an error:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43my\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# can line up trailing dimensions to make reading easier\n",
    "x=torch.empty(5,1,4,1)\n",
    "y=torch.empty(  3,1,1)\n",
    "print(f\"These two tensors are broadcastable \\n The resulting size is: {(x+y).size()} \\n\")\n",
    "\n",
    "# but not necessary:\n",
    "x=torch.empty(1)\n",
    "y=torch.empty(3,1,7)\n",
    "print(f\"These two tensors are broadcastable \\n The resulting size is: {(x+y).size()} \\n\")\n",
    "\n",
    "# The code below will give a runtime error\n",
    "x=torch.empty(5,2,4,1)\n",
    "y=torch.empty(3,1,1)\n",
    "print(f\"These two tensors are not broadcastable \\n Adding them together gives us an error:\")\n",
    "z = x+y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58JBSY4z0nRQ"
   },
   "source": [
    "# Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pL_yVaTjMpGD"
   },
   "source": [
    "Finally we will take a look at how the gradients work. This will become more clear during the lab as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ye2wZZv3M3P2"
   },
   "source": [
    "Autograd is an automatic differentiation system. Conceptually, autograd records a graph recording all of the operations that created the data as you execute operations on them. This gives you a graph whose leaves are the input tensors and roots are the output tensors. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule (see example below).\n",
    "\n",
    "Internally, autograd represents this graph as a graph of Function objects. When computing the forwards pass, autograd simultaneously performs the requested computations and builds up a graph representing the function that computes the gradient (the .grad_fn attribute of each torch.Tensor is an entry point into this graph). When the forwards pass is completed, we evaluate this graph in the backwards pass to compute the gradients.\n",
    "\n",
    "An example is elaborated below: by default a tensor's \"requires grad\" field is set to False. This means that no gradients will be calculated, thus the grad field is None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "STvA07aDMp4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "requires_grad: False\n",
      "grad: None\n",
      "tensor([[4., 3.],\n",
      "        [2., 1.]])\n",
      "requires_grad: False\n",
      "grad: None\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.FloatTensor([[1, 2],[3, 4]])\n",
    "print(tensor1)\n",
    "print(f\"requires_grad: {tensor1.requires_grad}\")\n",
    "print(f\"grad: {tensor1.grad}\")\n",
    "\n",
    "tensor2 = torch.FloatTensor([[4, 3],[2, 1]])\n",
    "print(tensor2)\n",
    "print(f\"requires_grad: {tensor2.requires_grad}\")\n",
    "print(f\"grad: {tensor2.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bp8dqB1JNEMT"
   },
   "source": [
    "When we set the requires_grad bool to True, the computational graph containing the order of functions will be saved as well. Lets see an example below where we compute the calculation $output = tensor1^2 * tensor2 + 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "3IFSfdkAMyBq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor1:\n",
      "\ttensor([[1., 2.],\n",
      "        [3., 4.]], requires_grad=True)\n",
      "tensor2:\n",
      "\ttensor([[4., 3.],\n",
      "        [2., 1.]])\n",
      "tensor3:\n",
      "\ttensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "\n",
      "\n",
      "intermidiate_output:\n",
      "\ttensor([[ 4., 12.],\n",
      "        [18., 16.]], grad_fn=<MulBackward0>)\n",
      "output:\n",
      "\ttensor([[ 5., 13.],\n",
      "        [19., 17.]], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "tensor1.grad:\n",
      "\ttensor([[ 8., 12.],\n",
      "        [12.,  8.]])\n"
     ]
    }
   ],
   "source": [
    "tensor1.requires_grad = True\n",
    "print(f\"tensor1:\\n\\t{tensor1}\")\n",
    "print(f\"tensor2:\\n\\t{tensor2}\")\n",
    "tensor3 = torch.ones_like(tensor1)\n",
    "print(f\"tensor3:\\n\\t{tensor3}\")\n",
    "\n",
    "intermidiate_output = torch.pow(tensor1, 2) * tensor2\n",
    "output = intermidiate_output + tensor3\n",
    "print(f\"\\n\\nintermidiate_output:\\n\\t{intermidiate_output}\")\n",
    "print(f\"output:\\n\\t{output}\")\n",
    "\n",
    "\n",
    "# Gradients are cumulated with each backprop, therefore it is safer to zero them out first\n",
    "tensor1.grad = torch.zeros_like(tensor1)\n",
    "output.backward(torch.ones_like(output))\n",
    "print(f\"\\n\\ntensor1.grad:\\n\\t{tensor1.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxcQVbNAgTyh"
   },
   "source": [
    "Note a few things here, first the backward function on the intermediate output is a multiplication (MulBackward0), whereas the output itself contains AddBackward0 as function. Second, note how we zero the grads out before running. Comment this line out and run the code above a few times. Notice how the gradients are added every time, thats why it it important to zero them out first. Thirdly, the backward function might seem a bit strange as we give \"torch.ones_like(output)\" as an argument... This is because we still have a matrix of dimensions 2x2 instead of one single number as output. More specifically, the loss is usual a scalar; in this case, you don't need to put `torch.ones_like(output)`.\n",
    "\n",
    "Because the output is a matrix of dimensions 2x2, we need to derrive the gradient at every position separately. Lets derive one of the 4 grad elements (1, 1) together:\n",
    "\n",
    "$\\frac{δ output}{δx} = \\frac{δ(x^2 * 4 + 1)}{δx} = 8x = 8$ (because x = 1)\n",
    "\n",
    "As can be seen here, we first derive the funtion with respect to a certain tensor, in our case `tensor1`. When we fill in the value of this tensor, we obtain the gradient of this forward pass with respect to that tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XJrZ-OIkuk5"
   },
   "source": [
    "Can you derrive the other elements as well?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
